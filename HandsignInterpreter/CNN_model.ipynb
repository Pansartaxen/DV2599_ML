{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3e43ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "336fecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \"\"\"Returns the data as a pandas dataframe\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv('training_data/sign_mnist_train.csv')\n",
    "    except:\n",
    "        df = pd.read_csv(r'HandsignInterpreter\\training_data\\sign_mnist_train.csv')\n",
    "    return df\n",
    "\n",
    "def get_test_data():\n",
    "    \"\"\"Returns the data as a pandas dataframe\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv('training_data/sign_mnist_test.csv')\n",
    "    except:\n",
    "        df = pd.read_csv(r'HandsignInterpreter\\training_data\\sign_mnist_test.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1a85c2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 75)        750       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 28, 28, 75)       300       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 75)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 50)        33800     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 14, 50)        0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 14, 14, 50)       200       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 50)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 25)          11275     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 7, 7, 25)         100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 25)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 400)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               205312    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24)                12312     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 264,049\n",
      "Trainable params: 263,749\n",
      "Non-trainable params: 300\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 14:50:44.367116: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "215/215 [==============================] - 31s 141ms/step - loss: 1.0394 - accuracy: 0.6719 - val_loss: 3.4429 - val_accuracy: 0.2163 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "215/215 [==============================] - 31s 146ms/step - loss: 0.2292 - accuracy: 0.9238 - val_loss: 1.6517 - val_accuracy: 0.5213 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "215/215 [==============================] - 31s 144ms/step - loss: 0.1068 - accuracy: 0.9647 - val_loss: 0.1262 - val_accuracy: 0.9636 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "215/215 [==============================] - 32s 149ms/step - loss: 0.0689 - accuracy: 0.9783 - val_loss: 0.1801 - val_accuracy: 0.9267 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "215/215 [==============================] - 33s 151ms/step - loss: 0.0500 - accuracy: 0.9840 - val_loss: 0.0538 - val_accuracy: 0.9803 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "215/215 [==============================] - 32s 147ms/step - loss: 0.0391 - accuracy: 0.9873 - val_loss: 0.0172 - val_accuracy: 0.9962 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "215/215 [==============================] - 32s 149ms/step - loss: 0.0332 - accuracy: 0.9897 - val_loss: 0.0161 - val_accuracy: 0.9953 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9903\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "215/215 [==============================] - 33s 154ms/step - loss: 0.0298 - accuracy: 0.9903 - val_loss: 0.0243 - val_accuracy: 0.9912 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "215/215 [==============================] - 35s 162ms/step - loss: 0.0156 - accuracy: 0.9950 - val_loss: 0.0086 - val_accuracy: 0.9971 - lr: 5.0000e-04\n",
      "Epoch 10/20\n",
      "215/215 [==============================] - 35s 163ms/step - loss: 0.0099 - accuracy: 0.9975 - val_loss: 0.0624 - val_accuracy: 0.9805 - lr: 5.0000e-04\n",
      "Epoch 11/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9962\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "215/215 [==============================] - 35s 165ms/step - loss: 0.0116 - accuracy: 0.9962 - val_loss: 0.0161 - val_accuracy: 0.9967 - lr: 5.0000e-04\n",
      "Epoch 12/20\n",
      "215/215 [==============================] - 34s 157ms/step - loss: 0.0083 - accuracy: 0.9976 - val_loss: 0.0077 - val_accuracy: 0.9968 - lr: 2.5000e-04\n",
      "Epoch 13/20\n",
      "215/215 [==============================] - 32s 148ms/step - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.0024 - val_accuracy: 0.9994 - lr: 2.5000e-04\n",
      "Epoch 14/20\n",
      "215/215 [==============================] - 33s 155ms/step - loss: 0.0070 - accuracy: 0.9978 - val_loss: 0.0047 - val_accuracy: 0.9993 - lr: 2.5000e-04\n",
      "Epoch 15/20\n",
      "215/215 [==============================] - 36s 166ms/step - loss: 0.0069 - accuracy: 0.9979 - val_loss: 0.0030 - val_accuracy: 0.9996 - lr: 2.5000e-04\n",
      "Epoch 16/20\n",
      "215/215 [==============================] - 35s 161ms/step - loss: 0.0067 - accuracy: 0.9983 - val_loss: 0.0186 - val_accuracy: 0.9936 - lr: 2.5000e-04\n",
      "Epoch 17/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 0.9983\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "215/215 [==============================] - 35s 164ms/step - loss: 0.0059 - accuracy: 0.9983 - val_loss: 0.0079 - val_accuracy: 0.9971 - lr: 2.5000e-04\n",
      "Epoch 18/20\n",
      "215/215 [==============================] - 34s 159ms/step - loss: 0.0051 - accuracy: 0.9988 - val_loss: 0.0014 - val_accuracy: 0.9997 - lr: 1.2500e-04\n",
      "Epoch 19/20\n",
      "215/215 [==============================] - 34s 156ms/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.0026 - val_accuracy: 0.9994 - lr: 1.2500e-04\n",
      "Epoch 20/20\n",
      "215/215 [==============================] - 34s 159ms/step - loss: 0.0052 - accuracy: 0.9985 - val_loss: 0.0014 - val_accuracy: 1.0000 - lr: 1.2500e-04\n",
      "225/225 [==============================] - 4s 19ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Accuracy of the model is -  100.0 %\n"
     ]
    }
   ],
   "source": [
    "df = get_data()\n",
    "df_test = get_test_data()\n",
    "classes_train = df['label']\n",
    "classes_test = df_test['label']\n",
    "del df['label']\n",
    "del df_test['label']\n",
    "\n",
    "label_binarizer = LabelBinarizer()\n",
    "classes_train = label_binarizer.fit_transform(classes_train)\n",
    "classes_test = label_binarizer.fit_transform(classes_test)\n",
    "\n",
    "vector_train = df.values\n",
    "vector_test = df_test.values\n",
    "vector_train = vector_train / 255\n",
    "vector_test = vector_test / 255\n",
    "vector_train = vector_train.reshape(-1, 28, 28, 1)\n",
    "vector_test = vector_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range = 0.1, # Randomly zoom image\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=False,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "\n",
    "datagen.fit(vector_train)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 512 , activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units = 24 , activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(datagen.flow(vector_train, classes_train, batch_size = 128), epochs = 20, validation_data = (vector_test, classes_test), callbacks = [learning_rate_reduction])\n",
    "print(\"Accuracy of the model is - \" , model.evaluate(vector_test, classes_test)[1]*100 , \"%\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4e8f03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Calling `save('my_model')` creates a SavedModel folder `my_model`.\n",
    "model.save(\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "176fd571",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "img = plt.imread('hand.png')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc38188d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array (28, 28, 1) Batch (1, 28, 28) Image (28, 28)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "img_array = image.img_to_array(img)\n",
    "img_batch = np.expand_dims(img, axis=0)\n",
    "print('Array',img_array.shape,'Batch',img_batch.shape,'Image',img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6f6f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "img_preprocessed = preprocess_input(img_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d324ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f5d526a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(img_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2afba8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6e22607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = list(prediction[0])\n",
    "for num in pred:\n",
    "    num = num.item()\n",
    "sorted_pred = sorted(pred, reverse=True)\n",
    "print(sorted_pred)\n",
    "print(pred)\n",
    "index = pred.index(sorted_pred[0])\n",
    "index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
