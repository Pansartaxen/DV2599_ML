{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba3e43ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "336fecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \"\"\"Returns the data as a pandas dataframe\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv('training_data/sign_mnist_train.csv')\n",
    "    except:\n",
    "        df = pd.read_csv(r'HandsignInterpreter\\training_data\\sign_mnist_train.csv')\n",
    "    return df\n",
    "\n",
    "def get_test_data():\n",
    "    \"\"\"Returns the data as a pandas dataframe\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv('training_data/sign_mnist_test.csv')\n",
    "    except:\n",
    "        df = pd.read_csv(r'HandsignInterpreter\\training_data\\sign_mnist_test.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1a85c2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 75)        750       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 28, 28, 75)       300       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 75)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 50)        33800     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 14, 14, 50)        0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 14, 14, 50)       200       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 7, 7, 50)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 7, 7, 25)          11275     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 7, 7, 25)         100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 4, 4, 25)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               205312    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 24)                12312     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 264,049\n",
      "Trainable params: 263,749\n",
      "Non-trainable params: 300\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "215/215 [==============================] - 34s 153ms/step - loss: 1.0536 - accuracy: 0.6717 - val_loss: 4.0834 - val_accuracy: 0.1401 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "215/215 [==============================] - 32s 150ms/step - loss: 0.2140 - accuracy: 0.9289 - val_loss: 1.2720 - val_accuracy: 0.6030 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "215/215 [==============================] - 34s 156ms/step - loss: 0.0938 - accuracy: 0.9698 - val_loss: 0.1347 - val_accuracy: 0.9582 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "215/215 [==============================] - 34s 159ms/step - loss: 0.0632 - accuracy: 0.9794 - val_loss: 0.1079 - val_accuracy: 0.9704 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "215/215 [==============================] - 33s 152ms/step - loss: 0.0433 - accuracy: 0.9862 - val_loss: 0.0349 - val_accuracy: 0.9905 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "215/215 [==============================] - 33s 151ms/step - loss: 0.0348 - accuracy: 0.9887 - val_loss: 0.0461 - val_accuracy: 0.9876 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9915\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "215/215 [==============================] - 34s 158ms/step - loss: 0.0275 - accuracy: 0.9915 - val_loss: 0.0577 - val_accuracy: 0.9815 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "215/215 [==============================] - 33s 155ms/step - loss: 0.0157 - accuracy: 0.9952 - val_loss: 0.0046 - val_accuracy: 0.9999 - lr: 5.0000e-04\n",
      "Epoch 9/20\n",
      "215/215 [==============================] - 34s 156ms/step - loss: 0.0120 - accuracy: 0.9961 - val_loss: 0.0118 - val_accuracy: 0.9961 - lr: 5.0000e-04\n",
      "Epoch 10/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.9972\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "215/215 [==============================] - 34s 156ms/step - loss: 0.0096 - accuracy: 0.9972 - val_loss: 0.0125 - val_accuracy: 0.9955 - lr: 5.0000e-04\n",
      "Epoch 11/20\n",
      "215/215 [==============================] - 33s 151ms/step - loss: 0.0095 - accuracy: 0.9973 - val_loss: 0.0041 - val_accuracy: 0.9990 - lr: 2.5000e-04\n",
      "Epoch 12/20\n",
      "215/215 [==============================] - 32s 150ms/step - loss: 0.0072 - accuracy: 0.9984 - val_loss: 0.0020 - val_accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 13/20\n",
      "215/215 [==============================] - 34s 160ms/step - loss: 0.0069 - accuracy: 0.9981 - val_loss: 7.0708e-04 - val_accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 14/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 0.9977\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "215/215 [==============================] - 35s 162ms/step - loss: 0.0076 - accuracy: 0.9977 - val_loss: 0.0026 - val_accuracy: 0.9992 - lr: 2.5000e-04\n",
      "Epoch 15/20\n",
      "215/215 [==============================] - 34s 159ms/step - loss: 0.0054 - accuracy: 0.9986 - val_loss: 0.0045 - val_accuracy: 0.9997 - lr: 1.2500e-04\n",
      "Epoch 16/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 0.9985\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "215/215 [==============================] - 34s 157ms/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.0015 - val_accuracy: 0.9999 - lr: 1.2500e-04\n",
      "Epoch 17/20\n",
      "215/215 [==============================] - 34s 160ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 5.4498e-04 - val_accuracy: 0.9999 - lr: 6.2500e-05\n",
      "Epoch 18/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 0.9990\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "215/215 [==============================] - 35s 161ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 5.9062e-04 - val_accuracy: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 19/20\n",
      "215/215 [==============================] - 34s 156ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 5.6955e-04 - val_accuracy: 0.9999 - lr: 3.1250e-05\n",
      "Epoch 20/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0037 - accuracy: 0.9991\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "215/215 [==============================] - 34s 159ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 4.6720e-04 - val_accuracy: 1.0000 - lr: 3.1250e-05\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 4.6720e-04 - accuracy: 1.0000\n",
      "Accuracy of the model is -  100.0 %\n"
     ]
    }
   ],
   "source": [
    "df = get_data()\n",
    "df_test = get_test_data()\n",
    "classes_train = df['label']\n",
    "classes_test = df_test['label']\n",
    "del df['label']\n",
    "del df_test['label']\n",
    "\n",
    "label_binarizer = LabelBinarizer()\n",
    "classes_train = label_binarizer.fit_transform(classes_train)\n",
    "classes_test = label_binarizer.fit_transform(classes_test)\n",
    "\n",
    "vector_train = df.values\n",
    "vector_test = df_test.values\n",
    "vector_train = vector_train / 255\n",
    "vector_test = vector_test / 255\n",
    "vector_train = vector_train.reshape(-1, 28, 28, 1)\n",
    "vector_test = vector_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range = 0.1, # Randomly zoom image\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=False,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "\n",
    "datagen.fit(vector_train)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 512 , activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units = 24 , activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(datagen.flow(vector_train, classes_train, batch_size = 128), epochs = 20, validation_data = (vector_test, classes_test), callbacks = [learning_rate_reduction])\n",
    "print(\"Accuracy of the model is - \" , model.evaluate(vector_test, classes_test)[1]*100 , \"%\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7bec6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: CNN_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: CNN_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "tf.keras.models.save_model(model, 'CNN_model')\n",
    "print('Model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19b377d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "model.save('cnn_model.h5')\n",
    "print('Model Saved!')\n",
    "# save model\n",
    "#model.save_weights('cnn_model-weights.h5')\n",
    "#print('Weights Saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e72f83cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Calling `save('my_model')` creates a SavedModel folder `my_model`.\n",
    "model.save(\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f3a4d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can be used to reconstruct the model identically.\n",
    "reconstructed_model = keras.models.load_model(\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435f911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412640d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check:\n",
    "np.testing.assert_allclose(\n",
    "    model.predict(test_input), reconstructed_model.predict(test_input)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
