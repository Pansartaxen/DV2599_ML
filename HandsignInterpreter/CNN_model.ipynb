{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba3e43ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "336fecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \"\"\"Returns the data as a pandas dataframe\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv('training_data/sign_mnist_train.csv')\n",
    "    except:\n",
    "        df = pd.read_csv(r'HandsignInterpreter\\training_data\\sign_mnist_train.csv')\n",
    "    return df\n",
    "\n",
    "def get_test_data():\n",
    "    \"\"\"Returns the data as a pandas dataframe\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv('training_data/sign_mnist_test.csv')\n",
    "    except:\n",
    "        df = pd.read_csv(r'HandsignInterpreter\\training_data\\sign_mnist_test.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1a85c2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 75)        750       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 28, 28, 75)       300       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 75)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 50)        33800     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 14, 14, 50)        0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 14, 14, 50)       200       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 7, 7, 50)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 7, 7, 25)          11275     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 7, 7, 25)         100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 4, 4, 25)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               205312    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 24)                12312     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 264,049\n",
      "Trainable params: 263,749\n",
      "Non-trainable params: 300\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "215/215 [==============================] - 34s 153ms/step - loss: 1.0536 - accuracy: 0.6717 - val_loss: 4.0834 - val_accuracy: 0.1401 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "215/215 [==============================] - 32s 150ms/step - loss: 0.2140 - accuracy: 0.9289 - val_loss: 1.2720 - val_accuracy: 0.6030 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "215/215 [==============================] - 34s 156ms/step - loss: 0.0938 - accuracy: 0.9698 - val_loss: 0.1347 - val_accuracy: 0.9582 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "215/215 [==============================] - 34s 159ms/step - loss: 0.0632 - accuracy: 0.9794 - val_loss: 0.1079 - val_accuracy: 0.9704 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "215/215 [==============================] - 33s 152ms/step - loss: 0.0433 - accuracy: 0.9862 - val_loss: 0.0349 - val_accuracy: 0.9905 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "215/215 [==============================] - 33s 151ms/step - loss: 0.0348 - accuracy: 0.9887 - val_loss: 0.0461 - val_accuracy: 0.9876 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9915\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "215/215 [==============================] - 34s 158ms/step - loss: 0.0275 - accuracy: 0.9915 - val_loss: 0.0577 - val_accuracy: 0.9815 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "215/215 [==============================] - 33s 155ms/step - loss: 0.0157 - accuracy: 0.9952 - val_loss: 0.0046 - val_accuracy: 0.9999 - lr: 5.0000e-04\n",
      "Epoch 9/20\n",
      "215/215 [==============================] - 34s 156ms/step - loss: 0.0120 - accuracy: 0.9961 - val_loss: 0.0118 - val_accuracy: 0.9961 - lr: 5.0000e-04\n",
      "Epoch 10/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.9972\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "215/215 [==============================] - 34s 156ms/step - loss: 0.0096 - accuracy: 0.9972 - val_loss: 0.0125 - val_accuracy: 0.9955 - lr: 5.0000e-04\n",
      "Epoch 11/20\n",
      "215/215 [==============================] - 33s 151ms/step - loss: 0.0095 - accuracy: 0.9973 - val_loss: 0.0041 - val_accuracy: 0.9990 - lr: 2.5000e-04\n",
      "Epoch 12/20\n",
      "215/215 [==============================] - 32s 150ms/step - loss: 0.0072 - accuracy: 0.9984 - val_loss: 0.0020 - val_accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 13/20\n",
      "215/215 [==============================] - 34s 160ms/step - loss: 0.0069 - accuracy: 0.9981 - val_loss: 7.0708e-04 - val_accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 14/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 0.9977\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "215/215 [==============================] - 35s 162ms/step - loss: 0.0076 - accuracy: 0.9977 - val_loss: 0.0026 - val_accuracy: 0.9992 - lr: 2.5000e-04\n",
      "Epoch 15/20\n",
      "215/215 [==============================] - 34s 159ms/step - loss: 0.0054 - accuracy: 0.9986 - val_loss: 0.0045 - val_accuracy: 0.9997 - lr: 1.2500e-04\n",
      "Epoch 16/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 0.9985\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "215/215 [==============================] - 34s 157ms/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.0015 - val_accuracy: 0.9999 - lr: 1.2500e-04\n",
      "Epoch 17/20\n",
      "215/215 [==============================] - 34s 160ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 5.4498e-04 - val_accuracy: 0.9999 - lr: 6.2500e-05\n",
      "Epoch 18/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 0.9990\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "215/215 [==============================] - 35s 161ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 5.9062e-04 - val_accuracy: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 19/20\n",
      "215/215 [==============================] - 34s 156ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 5.6955e-04 - val_accuracy: 0.9999 - lr: 3.1250e-05\n",
      "Epoch 20/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0037 - accuracy: 0.9991\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "215/215 [==============================] - 34s 159ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 4.6720e-04 - val_accuracy: 1.0000 - lr: 3.1250e-05\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 4.6720e-04 - accuracy: 1.0000\n",
      "Accuracy of the model is -  100.0 %\n"
     ]
    }
   ],
   "source": [
    "df = get_data()\n",
    "df_test = get_test_data()\n",
    "classes_train = df['label']\n",
    "classes_test = df_test['label']\n",
    "del df['label']\n",
    "del df_test['label']\n",
    "\n",
    "label_binarizer = LabelBinarizer()\n",
    "classes_train = label_binarizer.fit_transform(classes_train)\n",
    "classes_test = label_binarizer.fit_transform(classes_test)\n",
    "\n",
    "vector_train = df.values\n",
    "vector_test = df_test.values\n",
    "vector_train = vector_train / 255\n",
    "vector_test = vector_test / 255\n",
    "vector_train = vector_train.reshape(-1, 28, 28, 1)\n",
    "vector_test = vector_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range = 0.1, # Randomly zoom image\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=False,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "\n",
    "datagen.fit(vector_train)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 512 , activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units = 24 , activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(datagen.flow(vector_train, classes_train, batch_size = 128), epochs = 20, validation_data = (vector_test, classes_test), callbacks = [learning_rate_reduction])\n",
    "print(\"Accuracy of the model is - \" , model.evaluate(vector_test, classes_test)[1]*100 , \"%\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22661523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: CNN_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: CNN_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "tf.keras.models.save_model(model, 'CNN_model')\n",
    "print('Model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "328da99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "model.save('cnn_model.h5')\n",
    "print('Model Saved!')\n",
    "# save model\n",
    "#model.save_weights('cnn_model-weights.h5')\n",
    "#print('Weights Saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9270abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Calling `save('my_model')` creates a SavedModel folder `my_model`.\n",
    "model.save(\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d504829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can be used to reconstruct the model identically.\n",
    "reconstructed_model = keras.models.load_model(\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "72498902",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "img = plt.imread('hand.png')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b6f45376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array (28, 28, 1) Batch (1, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array = image.img_to_array(img)\n",
    "img_batch = np.expand_dims(img_array, axis=0)\n",
    "print('Array',img_array.shape,'Batch',img_batch.shape)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "563d132d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 3 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresnet50\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocess_input, decode_predictions\n\u001b[0;32m----> 2\u001b[0m img_preprocessed \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/applications/resnet.py:507\u001b[0m, in \u001b[0;36mpreprocess_input\u001b[0;34m(x, data_format)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeras.applications.resnet50.preprocess_input\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    505\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeras.applications.resnet.preprocess_input\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_input\u001b[39m(x, data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 507\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimagenet_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcaffe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/applications/imagenet_utils.py:114\u001b[0m, in \u001b[0;36mpreprocess_input\u001b[0;34m(x, data_format, mode)\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected data_format to be one of `channels_first` or \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    111\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`channels_last`. Received: data_format=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_format\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m--> 114\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_preprocess_numpy_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _preprocess_symbolic_input(\n\u001b[1;32m    118\u001b[0m       x, data_format\u001b[38;5;241m=\u001b[39mdata_format, mode\u001b[38;5;241m=\u001b[39mmode)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/applications/imagenet_utils.py:232\u001b[0m, in \u001b[0;36m_preprocess_numpy_input\u001b[0;34m(x, data_format, mode)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m   x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m mean[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 232\u001b[0m   x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m mean[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    233\u001b[0m   x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m mean[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    234\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m std \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 3 with size 1"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "img_preprocessed = preprocess_input(img_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f9b9adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "599097da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 97ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(img_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f6e6df24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.8276532e-11, 9.9873835e-01, 4.0645501e-13, 5.4525546e-13,\n",
       "        1.7356113e-07, 1.6505304e-08, 9.9790072e-12, 3.7996553e-10,\n",
       "        8.4454911e-08, 9.1597832e-12, 1.1784079e-10, 1.4691570e-09,\n",
       "        1.5091898e-07, 3.1238926e-12, 8.2114955e-09, 2.4326280e-10,\n",
       "        1.4681283e-10, 1.6006682e-11, 7.7292214e-11, 1.2611990e-03,\n",
       "        5.5791427e-10, 2.7145770e-08, 1.4802788e-09, 1.2680879e-09]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "34b66725",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`decode_predictions` expects a batch of predictions (i.e. a 2D array of shape (samples, 1000)). Found array with shape: (1, 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdecode_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/applications/resnet.py:514\u001b[0m, in \u001b[0;36mdecode_predictions\u001b[0;34m(preds, top)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeras.applications.resnet50.decode_predictions\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    512\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeras.applications.resnet.decode_predictions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_predictions\u001b[39m(preds, top\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m--> 514\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimagenet_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/applications/imagenet_utils.py:147\u001b[0m, in \u001b[0;36mdecode_predictions\u001b[0;34m(preds, top)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m CLASS_INDEX\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(preds\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n\u001b[0;32m--> 147\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`decode_predictions` expects \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma batch of predictions \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    149\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(i.e. a 2D array of shape (samples, 1000)). \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    150\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFound array with shape: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(preds\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m CLASS_INDEX \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m   fpath \u001b[38;5;241m=\u001b[39m data_utils\u001b[38;5;241m.\u001b[39mget_file(\n\u001b[1;32m    153\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet_class_index.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    154\u001b[0m       CLASS_INDEX_PATH,\n\u001b[1;32m    155\u001b[0m       cache_subdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    156\u001b[0m       file_hash\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc2c37ea517e94d9795004a39431a14cb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: `decode_predictions` expects a batch of predictions (i.e. a 2D array of shape (samples, 1000)). Found array with shape: (1, 24)"
     ]
    }
   ],
   "source": [
    "print(decode_predictions(prediction)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "343c2659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99873835, 0.001261199, 1.7356113e-07, 1.5091898e-07, 8.445491e-08, 2.714577e-08, 1.6505304e-08, 8.2114955e-09, 1.4802788e-09, 1.469157e-09, 1.2680879e-09, 5.5791427e-10, 3.7996553e-10, 2.432628e-10, 1.4681283e-10, 1.1784079e-10, 8.827653e-11, 7.7292214e-11, 1.6006682e-11, 9.979007e-12, 9.159783e-12, 3.1238926e-12, 5.4525546e-13, 4.06455e-13]\n",
      "[8.827653e-11, 0.99873835, 4.06455e-13, 5.4525546e-13, 1.7356113e-07, 1.6505304e-08, 9.979007e-12, 3.7996553e-10, 8.445491e-08, 9.159783e-12, 1.1784079e-10, 1.469157e-09, 1.5091898e-07, 3.1238926e-12, 8.2114955e-09, 2.432628e-10, 1.4681283e-10, 1.6006682e-11, 7.7292214e-11, 0.001261199, 5.5791427e-10, 2.714577e-08, 1.4802788e-09, 1.2680879e-09]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = list(prediction[0])\n",
    "for num in pred:\n",
    "    num = num.item()\n",
    "sorted_pred = sorted(pred, reverse=True)\n",
    "print(sorted_pred)\n",
    "print(pred)\n",
    "index = pred.index(sorted_pred[0])\n",
    "index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
